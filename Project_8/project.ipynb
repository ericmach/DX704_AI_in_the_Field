{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 6\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 0 ACTION -1 NEXT REWARD 0 NEXT STATE 135\n",
            "CURR STATE 135 ACTION 1 NEXT REWARD 0 NEXT STATE 256\n",
            "CURR STATE 256 ACTION 0 NEXT REWARD 0 NEXT STATE 384\n",
            "CURR STATE 384 ACTION 1 NEXT REWARD 0 NEXT STATE 513\n",
            "CURR STATE 513 ACTION 1 NEXT REWARD 0 NEXT STATE 650\n",
            "CURR STATE 650 ACTION 0 NEXT REWARD 0 NEXT STATE 658\n",
            "CURR STATE 658 ACTION 1 NEXT REWARD 0 NEXT STATE 667\n",
            "CURR STATE 667 ACTION 1 NEXT REWARD 0 NEXT STATE 548\n",
            "CURR STATE 548 ACTION 1 NEXT REWARD 0 NEXT STATE 421\n",
            "CURR STATE 421 ACTION 1 NEXT REWARD 0 NEXT STATE 286\n",
            "CURR STATE 286 ACTION 1 NEXT REWARD 0 NEXT STATE 279\n",
            "CURR STATE 279 ACTION 0 NEXT REWARD 0 NEXT STATE 399\n",
            "CURR STATE 399 ACTION 1 NEXT REWARD 0 NEXT STATE 512\n",
            "CURR STATE 512 ACTION 1 NEXT REWARD 0 NEXT STATE 641\n",
            "CURR STATE 641 ACTION 1 NEXT REWARD 0 NEXT STATE 778\n",
            "CURR STATE 778 ACTION -1 NEXT REWARD 0 NEXT STATE 785\n",
            "CURR STATE 785 ACTION 1 NEXT REWARD 0 NEXT STATE 922\n",
            "CURR STATE 922 ACTION -1 NEXT REWARD 0 NEXT STATE 929\n",
            "CURR STATE 929 ACTION 1 NEXT REWARD 0 NEXT STATE 1066\n",
            "CURR STATE 1066 ACTION -1 NEXT REWARD 0 NEXT STATE 1073\n",
            "CURR STATE 1073 ACTION -1 NEXT REWARD 0 NEXT STATE 1208\n",
            "CURR STATE 1208 ACTION 1 NEXT REWARD 0 NEXT STATE 1337\n",
            "CURR STATE 1337 ACTION 1 NEXT REWARD 0 NEXT STATE 1474\n",
            "CURR STATE 1474 ACTION -1 NEXT REWARD 0 NEXT STATE 1481\n",
            "CURR STATE 1481 ACTION 0 NEXT REWARD 0 NEXT STATE 1617\n",
            "CURR STATE 1617 ACTION 0 NEXT REWARD 0 NEXT STATE 1753\n",
            "CURR STATE 1753 ACTION -1 NEXT REWARD 0 NEXT STATE 1888\n",
            "CURR STATE 1888 ACTION 1 NEXT REWARD 0 NEXT STATE 2017\n",
            "CURR STATE 2017 ACTION -1 NEXT REWARD 0 NEXT STATE 2024\n",
            "CURR STATE 2024 ACTION -1 NEXT REWARD 0 NEXT STATE 2031\n",
            "CURR STATE 2031 ACTION 0 NEXT REWARD 0 NEXT STATE 2023\n",
            "CURR STATE 2023 ACTION -1 NEXT REWARD 0 NEXT STATE 2014\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 1990 ACTION -1 NEXT REWARD 0 NEXT STATE 1981\n",
            "CURR STATE 1981 ACTION 0 NEXT REWARD 0 NEXT STATE 1845\n",
            "CURR STATE 1845 ACTION 1 NEXT REWARD 0 NEXT STATE 1710\n",
            "CURR STATE 1710 ACTION -1 NEXT REWARD 0 NEXT STATE 1701\n",
            "CURR STATE 1701 ACTION 1 NEXT REWARD 0 NEXT STATE 1566\n",
            "CURR STATE 1566 ACTION -1 NEXT REWARD 0 NEXT STATE 1557\n",
            "CURR STATE 1557 ACTION 1 NEXT REWARD 0 NEXT STATE 1422\n",
            "CURR STATE 1422 ACTION 0 NEXT REWARD 0 NEXT STATE 1414\n",
            "CURR STATE 1414 ACTION 1 NEXT REWARD 0 NEXT STATE 1415\n",
            "CURR STATE 1415 ACTION -1 NEXT REWARD 0 NEXT STATE 1542\n",
            "CURR STATE 1542 ACTION 0 NEXT REWARD 0 NEXT STATE 1542\n",
            "CURR STATE 1542 ACTION 0 NEXT REWARD 0 NEXT STATE 1542\n",
            "CURR STATE 1542 ACTION 1 NEXT REWARD 0 NEXT STATE 1543\n",
            "CURR STATE 1543 ACTION -1 NEXT REWARD 0 NEXT STATE 1670\n",
            "CURR STATE 1670 ACTION 1 NEXT REWARD 0 NEXT STATE 1671\n",
            "CURR STATE 1671 ACTION -1 NEXT REWARD 0 NEXT STATE 1798\n",
            "CURR STATE 1798 ACTION 1 NEXT REWARD 0 NEXT STATE 1799\n",
            "CURR STATE 1799 ACTION 0 NEXT REWARD 0 NEXT STATE 1927\n",
            "CURR STATE 1927 ACTION -1 NEXT REWARD 0 NEXT STATE 1926\n",
            "CURR STATE 1926 ACTION 1 NEXT REWARD 0 NEXT STATE 1927\n",
            "CURR STATE 1927 ACTION 0 NEXT REWARD 0 NEXT STATE 1927\n",
            "CURR STATE 1927 ACTION -1 NEXT REWARD 0 NEXT STATE 1926\n",
            "CURR STATE 1926 ACTION -1 NEXT REWARD 0 NEXT STATE 1925\n",
            "CURR STATE 1925 ACTION 0 NEXT REWARD 0 NEXT STATE 1797\n",
            "CURR STATE 1797 ACTION -1 NEXT REWARD 0 NEXT STATE 1668\n",
            "CURR STATE 1668 ACTION 1 NEXT REWARD 0 NEXT STATE 1541\n",
            "CURR STATE 1541 ACTION -1 NEXT REWARD 0 NEXT STATE 1412\n",
            "CURR STATE 1412 ACTION 0 NEXT REWARD 0 NEXT STATE 1284\n",
            "CURR STATE 1284 ACTION -1 NEXT REWARD 0 NEXT STATE 1155\n",
            "CURR STATE 1155 ACTION 0 NEXT REWARD 0 NEXT STATE 1035\n",
            "CURR STATE 1035 ACTION -1 NEXT REWARD 0 NEXT STATE 914\n",
            "CURR STATE 914 ACTION 0 NEXT REWARD 0 NEXT STATE 922\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "with open(\"log-random.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\"])\n",
        "    \n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        writer.writerow([curr_state, curr_action, next_reward, next_state])\n",
        "        print(\"CURR STATE\", curr_state, \"ACTION\", curr_action,\n",
        "              \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "        #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "W8cFRd7uPGqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "Q = defaultdict(lambda: {a: 0.0 for a in simulator.get_actions(0)})\n",
        "\n",
        "alpha = 1.0      \n",
        "gamma = 0.9     \n",
        "\n",
        "with open(\"q-random.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "\n",
        "            old_value = Q[curr_state][curr_action]\n",
        "\n",
        "            next_max_q = max(Q[next_state].values())\n",
        "\n",
        "            new_value = next_reward + gamma * next_max_q\n",
        "\n",
        "            Q[curr_state][curr_action] = new_value\n",
        "\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "\n",
        "epsilon = 0.25\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    return max(actions, key=lambda a: Q[state][a])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "Q = defaultdict(lambda: {a: 0.0 for a in simulator.get_actions(0)})\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "with open(\"q-greedy.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"])\n",
        "\n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "\n",
        "            old_value = Q[curr_state][curr_action]\n",
        "            next_max_q = max(Q[next_state].values())\n",
        "\n",
        "            new_value = next_reward + gamma * next_max_q\n",
        "            Q[curr_state][curr_action] = new_value\n",
        "\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "with open(\"policy-greedy.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\", \"action\"])\n",
        "\n",
        "    for state in Q.keys():\n",
        "        best_action = max(Q[state], key=lambda a: Q[state][a])\n",
        "        writer.writerow([state, best_action])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "Q = defaultdict(lambda: {a: 0.0 for a in simulator.get_actions(0)})\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "episodes = 1000  \n",
        "\n",
        "epsilon_start = 0.5\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
        "\n",
        "    curr_state = random.choice(simulator.initial_states)\n",
        "\n",
        "    for step in range(1024):\n",
        "        actions = simulator.get_actions(curr_state)\n",
        "        curr_action = epsilon_greedy_policy(curr_state, actions, epsilon)\n",
        "\n",
        "        next_reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "\n",
        "        old_value = Q[curr_state][curr_action]\n",
        "        next_max = max(Q[next_state].values())\n",
        "        new_value = next_reward + gamma * next_max\n",
        "        Q[curr_state][curr_action] = new_value\n",
        "\n",
        "        curr_state = next_state\n",
        "\n",
        "        if next_reward > 0: \n",
        "            break\n",
        "\n",
        "total_states = simulator.resolution * simulator.resolution * 8 \n",
        "\n",
        "with open(\"policy-optimal.tsv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"state\", \"action\"])\n",
        "\n",
        "    for state in range(total_states):\n",
        "        if state in Q:\n",
        "            best_action = max(Q[state], key=lambda a: Q[state][a])\n",
        "        else:\n",
        "            best_action = 0\n",
        "        writer.writerow([state, best_action])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "None"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
